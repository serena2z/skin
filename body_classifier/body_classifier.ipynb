{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "#from shape import shape_to_mask\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob as glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import torchvision.transforms as tf\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# MODEL AND DATALOADER PARAMETERS\n",
    "Learning_Rate=0.001\n",
    "# decrease the learning rate after 25 epochs- learning rate decay\n",
    "width=height=224 # image width and height\n",
    "# keep original size for resnet\n",
    "batchSize=32\n",
    "# increase to 64\n",
    "numWorkers=10\n",
    "# increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, df, img_path = 'image_file', label_col = 'label', transforms=None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.label_col = label_col\n",
    "        self.img_path = img_path\n",
    "        self.transforms = transforms\n",
    "        #self.label_dict = {\"face\": 0, \"scalp\": 1, \"ear\": 2, \"neck\": 3, \"shoulders\": 4, \"arms_upper\": 5, \"arms_lower\": 6, \"hands\": 7, \"chest\": 8, \"abdomen\": 9, \"back_upper\": 10, \"back_lower\": 11, \"hips_and_glutes\": 12, \"genital_and_perianal\": 13, \"legs_upper\": 14, \"legs_lower\": 15, \"feet\": 16, \"closeup\": 17, \"dermoscope\": 18, \"non-skin\": 19 }\n",
    "        self.label_dict = {\"face\": 0, \"scalp\": 1, \"ear\": 2, \"neck\": 3, \"shoulders\": 4, \"arms_upper\": 5, \"arms_lower\": 6, \"hands\": 7, \"chest\": 8, \"abdomen\": 9, \"back_upper\": 10, \"back_lower\": 11, \"hips_and_glutes\": 12, \"genital_and_perianal\": 13, \"legs_upper\": 14, \"legs_lower\": 15, \"feet\": 16, \"dermoscope\": 17}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_location = self.df[self.img_path].iloc[index]\n",
    "        label = self.label_dict[self.df[self.label_col].iloc[index]]\n",
    "\n",
    "        try:\n",
    "            #print('img_location: ', img_location)\n",
    "            image = Image.open(img_location).convert('RGB')\n",
    "            # print image type\n",
    "            #print(type(' image_type: ', image, '\\n'))\n",
    "\n",
    "        except:\n",
    "            # choose another random image to load instead\n",
    "            random_idx = np.random.choice(self.df.shape[0])\n",
    "            print('img not found: ', img_location + '\\n')\n",
    "            #with open('none_importable_body_images.txt','a+') as fh:\n",
    "                #fh.write(img_location +',' + self.df[self.img_path].iloc[random_idx] + '\\n')\n",
    "            return self.__getitem__(random_idx)\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def classes(self):\n",
    "        return self.df[self.label_col].unique().tolist()\n",
    "    \n",
    "    def all_labels(self):\n",
    "        return self.df[self.label_col].tolist()\n",
    "        \n",
    "\n",
    "# affine transformations\n",
    "# vertical, jitter, horizontal flip, rotation, zoom, shear, etc\n",
    "data_transforms = {\n",
    "    'train': tf.Compose([\n",
    "        tf.Resize((height,width)),\n",
    "        tf.RandomHorizontalFlip(),\n",
    "        tf.RandomVerticalFlip(),\n",
    "        # tf.RandomRotation(30),\n",
    "        # tf.RandomAffine(0, shear=10),\n",
    "        # tf.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        tf.ToTensor(),\n",
    "        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': tf.Compose([\n",
    "        tf.Resize((height,width)),\n",
    "        tf.ToTensor(),\n",
    "        tf.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(csv_path, img_path):\n",
    "    df = pd.read_csv(csv_path, header=0)\n",
    "    df['image_file'] = img_path + '/' + df['image_file']\n",
    "    return df\n",
    "\n",
    "df_train = build_df('/share/pi/ogevaert/zhang/body_classifier/body_train_45k.csv',\n",
    "                    '/share/pi/ogevaert/sadee/skin/clinical/som-dermatology-photos-2020')\n",
    "\n",
    "df_val = build_df('/share/pi/ogevaert/zhang/body_classifier/body_val_45k.csv',\n",
    "                    '/share/pi/ogevaert/sadee/skin/clinical/som-dermatology-photos-2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {'train':  CustomImageDataset(df_train,label_col = 'body_label', transforms = data_transforms['train']),\n",
    "                  'val':    CustomImageDataset(df_val,label_col = 'body_label', transforms = data_transforms['val'])}\n",
    "\n",
    "# undersample the classes with the largest number of samples\n",
    "labels = image_datasets['train'].all_labels()\n",
    "label_dict = {\"face\": 0, \"scalp\": 1, \"ear\": 2, \"neck\": 3, \"shoulders\": 4, \"arms_upper\": 5, \"arms_lower\": 6, \"hands\": 7, \"chest\": 8, \"abdomen\": 9, \"back_upper\": 10, \"back_lower\": 11, \"hips_and_glutes\": 12, \"genital_and_perianal\": 13, \"legs_upper\": 14, \"legs_lower\": 15, \"feet\": 16, \"dermoscope\": 17}\n",
    "# map labels to integers\n",
    "labels = [label_dict[label] for label in labels]\n",
    "\n",
    "# SAMPLING TOP 3\n",
    "# class_counts = torch.bincount(torch.tensor(labels))\n",
    "# # Get the top 3 classes with the highest counts\n",
    "# top_classes = torch.argsort(class_counts, descending=True)[:3]\n",
    "# # Calculate weights for each sample\n",
    "# weights = 1.0 / class_counts\n",
    "# # Assign higher weights to samples from the top 3 classes, and 1.0 to the rest\n",
    "# sample_weights = torch.ones(len(labels))\n",
    "# for class_idx in top_classes:\n",
    "#     class_samples = torch.nonzero(torch.tensor(labels) == class_idx.item()).flatten()\n",
    "#     sample_weights[class_samples] = weights[class_idx]\n",
    "# print(sample_weights)\n",
    "# # Create a WeightedRandomSampler with the sample weights\n",
    "# sampler = WeightedRandomSampler(sample_weights, num_samples=len(labels), replacement=True)\n",
    "\n",
    "# SAMPLING ALL\n",
    "class_counts = torch.bincount(torch.tensor(labels))\n",
    "# Calculate weights for each sample\n",
    "weights = 1.0 / class_counts\n",
    "# Create a list of weights for each sample in the dataset\n",
    "sample_weights = [weights[label] for label in labels]\n",
    "# Create a WeightedRandomSampler with the sample weights\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=len(labels), replacement=True)\n",
    "\n",
    "dataloaders = {x: DataLoader(image_datasets[x], batch_size=batchSize,sampler = sampler, num_workers=numWorkers) for x in ['train', 'val']}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "#--------------Load and set net and optimizer-------------------------------------\n",
    "#print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# load resnet34 model\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "num_ftrs = model.fc.in_features\n",
    "# model.fc = torch.nn.Linear(num_ftrs, out_features=18, bias=True)\n",
    "model.fc = torch.nn.Linear(num_ftrs, out_features=18, bias=True)\n",
    "\n",
    "# freeze the first 3 layers of the model\n",
    "# ct = 0\n",
    "# for child in model.children():\n",
    "#     ct += 1\n",
    "#     if ct < 15:\n",
    "#         for param in child.parameters():\n",
    "#             param.requires_grad = False\n",
    "        \n",
    "    \n",
    "model=model.to(device)\n",
    "optimizer=torch.optim.Adam(params=model.parameters(),lr=Learning_Rate) # Create adam optimizer\n",
    "# add gamma parameter for learning rate decay - 25 epochs, gamma=0.1\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)\n",
    "class_w = image_datasets['train'].classes()\n",
    "all_labels = image_datasets['train'].all_labels()\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=class_w,  y=all_labels)\n",
    "class_weights = torch.FloatTensor(class_weights).cuda()\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights) # Create loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "   running_loss = 0.\n",
    "   running_corrects = 0.\n",
    "   for i, data in enumerate(tqdm(dataloaders['train'])):\n",
    "        # Every data instance is an input + label pair\n",
    "      images, labels = data\n",
    "      images = images.to(device)\n",
    "      labels = labels.to(device)\n",
    "\n",
    "      # Zero your gradients for every batch!\n",
    "      optimizer.zero_grad()\n",
    "      # Make predictions for this batch\n",
    "      outputs = model(images)\n",
    "      _, preds = torch.max(outputs, 1)\n",
    "      # Compute the loss and its gradients\n",
    "      loss = loss_fn(outputs, labels)\n",
    "      #loss.requires_grad = True\n",
    "      loss.backward()\n",
    "      # Adjust learning weights\n",
    "      optimizer.step()\n",
    "      # Gather data and report per 100 batches\n",
    "      running_loss += loss.detach().item() * images.size(0)\n",
    "      running_corrects += torch.sum(preds == labels.data).detach()\n",
    "      # if i % 100 == 99:\n",
    "      #    last_loss = running_loss / 100 # loss per batch\n",
    "      #    print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "      #    running_loss = 0.\n",
    "   \n",
    "   # update the scheduler\n",
    "   scheduler.step()\n",
    "   # calculate loss for epoch\n",
    "   avg_loss = running_loss / dataset_sizes['train']\n",
    "   avg_correct = running_corrects / dataset_sizes['train']\n",
    "   return avg_loss, avg_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "EPOCHS = 120\n",
    "epoch_number = 0\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss, avg_corrects = train_one_epoch(epoch_number)\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    running_vcorrects = 0.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(tqdm(dataloaders['val'])):\n",
    "            vimages, vlabels = vdata\n",
    "            vimages = vimages.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs = model(vimages)\n",
    "            _, vpreds = torch.max(voutputs, 1)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss.detach().item() * vimages.size(0)\n",
    "            running_vcorrects += torch.sum(vpreds == vlabels.data).detach()\n",
    "\n",
    "    avg_vloss = running_vloss / dataset_sizes['val']\n",
    "    avg_vcorrects = running_vcorrects / dataset_sizes['val']\n",
    "    print('LOSS train {} train acc {} valid {} valid acc {}'.format(avg_loss, avg_corrects, avg_vloss, avg_vcorrects))\n",
    "    \n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = '/share/pi/ogevaert/zhang/SkinSegmentation/models/body_0609_45k/body_model_best_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # save model every 10 epochs\n",
    "    if (epoch_number+1) % 10 == 0:\n",
    "        model_path = '/share/pi/ogevaert/zhang/SkinSegmentation/models/body_0609_45k/body_model_{}_{}'.format(timestamp, epoch_number)\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
